import cv2
import numpy as np
import os
from pathlib import Path
import time
import subprocess
from ultralytics import YOLO
from typing import List, Tuple, Optional

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ =====================
# äººè„¸ç­›é€‰é…ç½®
FACE_SIZE_THRESHOLD = 0.15    # äººè„¸å æ¯”é˜ˆå€¼ï¼ˆç›¸å¯¹å¸§å®½é«˜ï¼‰
ALLOWED_FACE_COUNT = 1       # ä»…å…è®¸å•äººè„¸
DET_SCORE_THRESHOLD = 0.7    # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
BATCH_SIZE = 8               # æ‰¹é‡å¤„ç†å¤§å°
# è§†é¢‘å¤„ç†é…ç½®
FRAME_SKIP = 0               # å¸§è·³è¿‡æ•°ï¼ˆ0=é€å¸§æ£€æµ‹ï¼‰
MIN_VALID_DURATION = 5       # æœ€å°åˆæ ¼ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
VIDEO_FPS = 0                # 0=ä½¿ç”¨åŸè§†é¢‘FPS
SPEED_PRINT_INTERVAL = 50    # é€Ÿåº¦æ‰“å°é—´éš”ï¼ˆå¸§ï¼‰
TOLERANCE_FRAMES = 5         # è¿ç»­ä¸åˆæ ¼å¸§æ•°å®¹é”™é˜ˆå€¼ï¼ˆä»…é€å¸§æ—¶ç”Ÿæ•ˆï¼‰

# ===================== åˆå§‹åŒ–YOLOæ¨¡å‹ =====================
def init_yolo_model(model_path: str = "../yolov8l_100e.pt") -> YOLO:
    """åˆå§‹åŒ–YOLOv8äººè„¸æ£€æµ‹æ¨¡å‹ï¼ˆå…¼å®¹ä¸åŒultralyticsç‰ˆæœ¬ï¼‰"""
    try:
        model = YOLO(model_path)
        # å…¼å®¹å¼è·å–è®¾å¤‡ä¿¡æ¯ï¼ˆä¼˜å…ˆç”¨predictor.deviceï¼Œå¤‡ç”¨torchåˆ¤æ–­ï¼‰
        if hasattr(model, 'predictor') and hasattr(model.predictor, 'device'):
            device = model.predictor.device
        else:
            import torch
            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # æ ¼å¼åŒ–è¾“å‡ºè®¾å¤‡ä¿¡æ¯
        device_str = '0' if str(device).startswith('cuda') else 'cpu'
        print(f"ğŸ”§ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device_str}")
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


# ===================== æ ¸å¿ƒç­›é€‰å‡½æ•° =====================
def is_high_quality_face(boxes_obj, img_w: int, img_h: int) -> Tuple[bool, str]:
    """
    åˆ¤æ–­äººè„¸æ˜¯å¦ç¬¦åˆè´¨é‡è¦æ±‚ï¼ˆæ•°é‡+å æ¯”ï¼‰
    :param boxes_obj: ultralytics.engine.results.Boxes å¯¹è±¡ï¼ˆåŒ…å«åæ ‡ã€ç½®ä¿¡åº¦ã€ç±»åˆ«ï¼‰
    :param img_w: å¸§å®½åº¦
    :param img_h: å¸§é«˜åº¦
    :return: (æ˜¯å¦åˆæ ¼, åŸå› )
    """
    """
    # æµ‹è¯•äººè„¸å°ºå¯¸
    for i in range(len(scores)):
        x1i, y1i, x2i, y2i = coords[i]
        face_wi = x2i- x1i
        face_hi = y2i - y1i
        face_w_ratioi = face_wi / img_w
        face_h_ratioi = face_hi / img_h
        print(f"    ç¬¬{i}å¼ äººè„¸å æ¯” face_w_ratio: {face_w_ratioi}; face_h_ratio: {face_h_ratioi}")
    """
    # 1. ç©ºæ£€æµ‹ç»“æœç›´æ¥è¿”å›ä¸åˆæ ¼
    if len(boxes_obj) == 0:
        return False, "æœªæ£€æµ‹åˆ°äººè„¸"
    
    # 2. ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤ä½ç½®ä¿¡åº¦äººè„¸ï¼ˆç½®ä¿¡åº¦é˜ˆå€¼ä¼˜å…ˆï¼‰
    coords = boxes_obj.xyxy.cpu().numpy()       # æ‰€æœ‰æ£€æµ‹æ¡†åæ ‡ (N,4)
    scores = boxes_obj.conf.cpu().numpy()       # æ‰€æœ‰æ£€æµ‹æ¡†ç½®ä¿¡åº¦ (N,)
    print(f"    åŸå§‹æ£€æµ‹äººè„¸æ•°: {len(scores)}, ç½®ä¿¡åº¦åˆ—è¡¨: {scores.round(2)}")
    # æµ‹è¯•äººè„¸å°ºå¯¸
    for i in range(len(scores)):
        x1i, y1i, x2i, y2i = coords[i]
        face_wi = x2i- x1i
        face_hi = y2i - y1i
        face_w_ratioi = face_wi / img_w
        face_h_ratioi = face_hi / img_h
        print("     åŸå§‹æ£€æµ‹äººè„¸çš„æ¯”ä¾‹")
        print(f"    ç¬¬{i}å¼ äººè„¸å æ¯” face_w_ratio: {face_w_ratioi}; face_h_ratio: {face_h_ratioi}")
    
    # è¿‡æ»¤ä½ç½®ä¿¡åº¦äººè„¸
    conf_mask = scores >= DET_SCORE_THRESHOLD
    conf_valid_coords = coords[conf_mask]       # ç½®ä¿¡åº¦è¾¾æ ‡çš„åæ ‡
    conf_valid_scores = scores[conf_mask]       # ç½®ä¿¡åº¦è¾¾æ ‡çš„åˆ†æ•°
    if len(conf_valid_coords) == 0:
        return False, f"æ— ç½®ä¿¡åº¦è¾¾æ ‡äººè„¸ï¼ˆé˜ˆå€¼={DET_SCORE_THRESHOLD}ï¼‰"
    
    # 3. ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤å°ºå¯¸ä¸è¾¾æ ‡äººè„¸ï¼ˆåœ¨ç½®ä¿¡åº¦åˆæ ¼çš„åŸºç¡€ä¸Šï¼‰
    size_valid_coords = []
    size_valid_scores = []
    for i in range(len(conf_valid_coords)):
        x1i, y1i, x2i, y2i = conf_valid_coords[i]
        face_wi = x2i - x1i
        face_hi = y2i - y1i
        face_w_ratioi = face_wi / img_w
        face_h_ratioi = face_hi / img_h
        print(f"    ç½®ä¿¡åº¦è¾¾æ ‡äººè„¸{i} - å æ¯”å®½: {face_w_ratioi:.2f}, å æ¯”é«˜: {face_h_ratioi:.2f}")
        
        # å°ºå¯¸è¾¾æ ‡åˆ™ä¿ç•™
        if face_w_ratioi >= FACE_SIZE_THRESHOLD and face_h_ratioi >= FACE_SIZE_THRESHOLD:
            size_valid_coords.append(conf_valid_coords[i])
            size_valid_scores.append(conf_valid_scores[i])
        else:
            print(f"    äººè„¸{i}å°ºå¯¸ä¸è¾¾æ ‡ï¼ˆé˜ˆå€¼={FACE_SIZE_THRESHOLD}ï¼‰ï¼Œè¿‡æ»¤")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆæ–¹ä¾¿åç»­å¤„ç†ï¼‰
    size_valid_coords = np.array(size_valid_coords)
    size_valid_scores = np.array(size_valid_scores)
    if len(size_valid_coords) == 0:
        return False, f"ç½®ä¿¡åº¦è¾¾æ ‡ä½†æ— å°ºå¯¸åˆæ ¼äººè„¸ï¼ˆå æ¯”é˜ˆå€¼={FACE_SIZE_THRESHOLD}ï¼‰"
    
    # 4. ç¬¬ä¸‰æ­¥ï¼šåˆ¤æ–­äººè„¸æ•°é‡ï¼ˆæœ€ååˆ¤æ–­æ•°é‡ï¼‰
    if len(size_valid_coords) != ALLOWED_FACE_COUNT:
        return False, f"å°ºå¯¸+ç½®ä¿¡åº¦è¾¾æ ‡äººè„¸æ•°={len(size_valid_coords)}ï¼ˆè¦æ±‚{ALLOWED_FACE_COUNT}å¼ ï¼‰"
    
    # æ‰€æœ‰æ¡ä»¶è¾¾æ ‡
    final_face = size_valid_coords[0]
    face_w = final_face[2] - final_face[0]
    face_h = final_face[3] - final_face[1]
    face_w_ratio = face_w / img_w
    face_h_ratio = face_h / img_h
    print(f"    æœ€ç»ˆåˆæ ¼äººè„¸ - å æ¯”å®½: {face_w_ratio:.2f}, å æ¯”é«˜: {face_h_ratio:.2f}, ç½®ä¿¡åº¦: {size_valid_scores[0]:.2f}")
    
    return True, "é«˜è´¨é‡äººè„¸ï¼ˆç½®ä¿¡åº¦+å°ºå¯¸+æ•°é‡å‡è¾¾æ ‡ï¼‰"


def get_frame_timestamp(frame_idx: int, fps: float) -> float:
    """å°†å¸§ç´¢å¼•è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆç§’ï¼‰"""
    return frame_idx / fps

def cut_video_by_timestamp(input_path: str, output_path: str, start_ts: float, end_ts: float) -> bool:
    """ä½¿ç”¨ffmpegè£å‰ªè§†é¢‘ï¼ˆä¿ç•™éŸ³é¢‘ï¼‰"""
    duration = end_ts - start_ts
    if duration < MIN_VALID_DURATION:
        print(f"âš ï¸  ç‰‡æ®µæ—¶é•¿{duration:.2f}ç§’ < æœ€å°é˜ˆå€¼{MIN_VALID_DURATION}ç§’ï¼Œè·³è¿‡ä¿å­˜")
        return False
    
    cmd = [
        "ffmpeg",
        "-ss", str(start_ts),
        "-i", input_path,
        "-to", str(end_ts),
        "-c:v", "copy",
        "-c:a", "copy",
        "-y",
        "-loglevel", "error",
        output_path
    ]
    
    try:
        subprocess.run(cmd, check=True)
        print(f"âœ… ä¿å­˜ç‰‡æ®µï¼š{output_path}ï¼ˆæ—¶é•¿ï¼š{duration:.2f}ç§’ï¼‰")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ è£å‰ªå¤±è´¥ï¼š{output_path}ï¼Œé”™è¯¯ï¼š{e}")
        return False

def process_video(video_path: str, output_dir: str = ".", model: YOLO = None) -> None:
    """å¤„ç†è§†é¢‘ï¼ˆæ‰¹é‡æ£€æµ‹+ç‰‡æ®µè£å‰ªï¼‰"""
    if model is None:
        model = init_yolo_model()
    
    os.makedirs(output_dir, exist_ok=True)
    video_name = Path(video_path).stem
    cap = cv2.VideoCapture(video_path)
    
    # è·å–è§†é¢‘åŸºç¡€ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS) if VIDEO_FPS == 0 else VIDEO_FPS
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total_duration = total_frames / fps if fps > 0 else 0
    
    if total_frames == 0:
        print(f"é”™è¯¯ï¼šæ— æ³•è¯»å–è§†é¢‘ {video_path}")
        return
    
    # åˆå§‹åŒ–å˜é‡
    clip_num = 0
    frame_idx = 0
    processed_frames = 0
    start_time = time.time()
    valid_clip_start_ts: Optional[float] = None
    consecutive_invalid = 0
    batch_frames: List[np.ndarray] = []
    batch_indices: List[int] = []  # è®°å½•æ‰¹æ¬¡ä¸­å¸§çš„åŸå§‹ç´¢å¼•
    
    print(f"ğŸ“½ï¸  å¼€å§‹å¤„ç†ï¼š{video_path}")
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯ï¼šFPS={fps:.2f}, åˆ†è¾¨ç‡={width}x{height}, æ€»å¸§æ•°={total_frames}, æ€»æ—¶é•¿={total_duration:.2f}ç§’")
    print(f"âš™ï¸  é…ç½®ï¼šç½®ä¿¡åº¦={DET_SCORE_THRESHOLD}, æ‰¹é‡å¤§å°={BATCH_SIZE}, è·³å¸§æ•°={FRAME_SKIP}, æœ€å°ç‰‡æ®µæ—¶é•¿={MIN_VALID_DURATION}ç§’")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # å¸§è·³è¿‡å¤„ç†
        if frame_idx % (FRAME_SKIP + 1) != 0:
            frame_idx += 1
            continue
        
        # æ”¶é›†å¸§åˆ°æ‰¹æ¬¡
        batch_frames.append(frame)
        batch_indices.append(frame_idx)
        
        # æ‰¹æ¬¡æ»¡äº†æˆ–æœ€åä¸€æ‰¹ä¸è¶³æ‰¹é‡å¤§å°æ—¶è¿›è¡Œæ£€æµ‹
        if len(batch_frames) >= BATCH_SIZE:
            # æ‰¹é‡æ£€æµ‹
            results = model(batch_frames, verbose=False)
            # print(f"batch_frames' results: \n{results}")
            
            # å¤„ç†æ‰¹æ¬¡ç»“æœ
            for i, (result, frame_idx_batch) in enumerate(zip(results, batch_indices)):
                # boxes = result.boxes.xyxy.cpu().numpy()  # æ ¼å¼ï¼š[x1, y1, x2, y2, score]
                # print(f"result: \n{result}")
                # print(f"result.boxes: \n{result.boxes}")
                is_valid, reason = is_high_quality_face(result.boxes, width, height)
                
                # å¤„ç†ç‰‡æ®µé€»è¾‘
                current_ts = get_frame_timestamp(frame_idx_batch, fps)
                
                if is_valid:
                    consecutive_invalid = 0
                    if valid_clip_start_ts is None:
                        valid_clip_start_ts = current_ts
                        print(f"ğŸ”„ å¼€å§‹åˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={valid_clip_start_ts:.2f}ç§’ï¼‰")
                else:
                    # å¤„ç†ä¸åˆæ ¼å¸§
                    if FRAME_SKIP == 0:  # é€å¸§æ¨¡å¼
                        consecutive_invalid += 1
                        if valid_clip_start_ts is not None and consecutive_invalid > TOLERANCE_FRAMES:
                            end_ts = get_frame_timestamp(frame_idx_batch - consecutive_invalid, fps)
                            output_path = os.path.join(output_dir, f"{video_name}_croped{clip_num}.mp4")
                            print(f"è£å‰ªå‘½ä»¤ï¼šå¼€å§‹æ—¶é—´={valid_clip_start_ts:.2f}ç§’ï¼Œç»“æŸæ—¶é—´={end_ts:.2f}ç§’")
                            if cut_video_by_timestamp(video_path, output_path, valid_clip_start_ts, end_ts):
                                clip_num += 1
                            valid_clip_start_ts = None
                            consecutive_invalid = 0
                            print(f"ğŸ”š ç»“æŸåˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={current_ts:.2f}ç§’ï¼‰ï¼ŒåŸå› ï¼š{reason}")
                    else:  # è·³å¸§æ¨¡å¼
                        if valid_clip_start_ts is not None:
                            # ä»¥ä¸Šä¸€ä¸ªæ£€æµ‹å¸§æ—¶é—´ä½œä¸ºç»“æŸ
                            end_ts = get_frame_timestamp(batch_indices[i-1] if i > 0 else frame_idx_batch, fps)
                            output_path = os.path.join(output_dir, f"{video_name}_croped{clip_num}.mp4")
                            if cut_video_by_timestamp(video_path, output_path, valid_clip_start_ts, end_ts):
                                clip_num += 1
                            valid_clip_start_ts = None
                            print(f"ğŸ”š ç»“æŸåˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={current_ts:.2f}ç§’ï¼‰ï¼ŒåŸå› ï¼š{reason}")
                
                # æ‰“å°å¸§ä¿¡æ¯
                status = "âœ…" if is_valid else "âŒ"
                print(f"å¸§{frame_idx_batch} {status} - {reason}")
                
                processed_frames += 1
                # é€Ÿåº¦ç»Ÿè®¡
                if processed_frames % SPEED_PRINT_INTERVAL == 0:
                    elapsed = time.time() - start_time
                    speed = processed_frames / elapsed
                    print(f"ğŸ“ˆ å·²å¤„ç†{processed_frames}å¸§ï¼Œé€Ÿåº¦ï¼š{speed:.2f}å¸§/ç§’")
            
            # é‡ç½®æ‰¹æ¬¡
            batch_frames = []
            batch_indices = []
        
        frame_idx += 1

    # å¤„ç†æœ€åä¸€æ‰¹å‰©ä½™å¸§
    if batch_frames:
        results = model(batch_frames, verbose=False)
        for i, (result, frame_idx_batch) in enumerate(zip(results, batch_indices)):
            is_valid, reason = is_high_quality_face(result.boxes, width, height)
            current_ts = get_frame_timestamp(frame_idx_batch, fps)
            
            if is_valid and valid_clip_start_ts is None:
                valid_clip_start_ts = current_ts
            
            processed_frames += 1

    # å¤„ç†æœ€åä¸€æ®µåˆæ ¼ç‰‡æ®µ
    if valid_clip_start_ts is not None:
        output_path = os.path.join(output_dir, f"{video_name}_croped{clip_num}.mp4")
        cut_video_by_timestamp(video_path, output_path, valid_clip_start_ts, total_duration)
        clip_num += 1

    # æ”¶å°¾ç»Ÿè®¡
    total_elapsed = time.time() - start_time
    avg_speed = processed_frames / total_elapsed if total_elapsed > 0 else 0
    print(f"\nğŸ å¤„ç†å®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶ï¼š{total_elapsed:.2f}ç§’ï¼Œå¹³å‡é€Ÿåº¦ï¼š{avg_speed:.2f}å¸§/ç§’")
    print(f"ğŸ“¦ ç”Ÿæˆåˆæ ¼ç‰‡æ®µæ•°ï¼š{clip_num}ï¼ˆä¿å­˜è·¯å¾„ï¼š{os.path.abspath(output_dir)}ï¼‰")

    cap.release()

# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    test_video_path = "24494339-1-192.mp4"
    output_directory = "./output/24494339-1-192_yolov8l_100e_test3"
    
    # cut_video_by_timestamp(test_video_path,'./test.mp4',20,30)
    # # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
    # try:
    #     subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
    # except (subprocess.CalledProcessError, FileNotFoundError):
    #     print(f"âŒ æœªæ‰¾åˆ°FFmpegï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶åŠ å…¥ç¯å¢ƒå˜é‡")
    #     exit(1)
    
    # # åˆå§‹åŒ–æ¨¡å‹å¹¶å¤„ç†è§†é¢‘
    # model = init_yolo_model()
    # process_video(test_video_path, output_directory, model)
