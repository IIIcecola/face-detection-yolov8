import cv2
import numpy as np
import os
from pathlib import Path
import time
import subprocess
from ultralytics import YOLO
from typing import List, Tuple, Optional, Dict, Any
import json

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ =====================
# äººè„¸ç­›é€‰é…ç½®
FACE_SIZE_THRESHOLD = 0.15    # äººè„¸å æ¯”é˜ˆå€¼ï¼ˆç›¸å¯¹å¸§å®½é«˜ï¼‰
ALLOWED_FACE_COUNT = 1       # ä»…å…è®¸å•äººè„¸
DET_SCORE_THRESHOLD = 0.7    # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
BATCH_SIZE = 8               # æ‰¹é‡å¤„ç†å¤§å°
# è§†é¢‘å¤„ç†é…ç½®
FRAME_SKIP = 0               # å¸§è·³è¿‡æ•°ï¼ˆ0=é€å¸§æ£€æµ‹ï¼‰
MIN_VALID_DURATION = 5       # æœ€å°åˆæ ¼ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
VIDEO_FPS = 0                # 0=ä½¿ç”¨åŸè§†é¢‘FPS
SPEED_PRINT_INTERVAL = 50    # é€Ÿåº¦æ‰“å°é—´éš”ï¼ˆå¸§ï¼‰
TOLERANCE_FRAMES = 5         # è¿ç»­ä¸åˆæ ¼å¸§æ•°å®¹é”™é˜ˆå€¼ï¼ˆä»…é€å¸§æ—¶ç”Ÿæ•ˆï¼‰

# è£å‰ªé…ç½®
CROP_PADDING_RATIO = 0.1     # è£å‰ªæ¡†é¢å¤–å¡«å……æ¯”ä¾‹ï¼ˆ10%ï¼‰
MIN_CROP_SIZE = 64           # æœ€å°è£å‰ªå°ºå¯¸ï¼ˆåƒç´ ï¼‰

# ===================== åˆå§‹åŒ–YOLOæ¨¡å‹ =====================
def init_yolo_model(model_path: str = "yolov8l_100e.pt") -> YOLO:
    """åˆå§‹åŒ–YOLOv8äººè„¸æ£€æµ‹æ¨¡å‹ï¼ˆå…¼å®¹ä¸åŒultralyticsç‰ˆæœ¬ï¼‰"""
    try:
        model = YOLO(model_path)
        # å…¼å®¹å¼è·å–è®¾å¤‡ä¿¡æ¯ï¼ˆä¼˜å…ˆç”¨predictor.deviceï¼Œå¤‡ç”¨torchåˆ¤æ–­ï¼‰
        if hasattr(model, 'predictor') and hasattr(model.predictor, 'device'):
            device = model.predictor.device
        else:
            import torch
            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # æ ¼å¼åŒ–è¾“å‡ºè®¾å¤‡ä¿¡æ¯
        device_str = '0' if str(device).startswith('cuda') else 'cpu'
        print(f"ğŸ”§ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device_str}")
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


# ===================== æ ¸å¿ƒç­›é€‰å‡½æ•° =====================
def is_high_quality_face(boxes_obj, img_w: int, img_h: int) -> Tuple[bool, str, Optional[np.ndarray]]:
    """
    åˆ¤æ–­äººè„¸æ˜¯å¦ç¬¦åˆè´¨é‡è¦æ±‚ï¼ˆæ•°é‡+å æ¯”ï¼‰
    :param boxes_obj: ultralytics.engine.results.Boxes å¯¹è±¡ï¼ˆåŒ…å«åæ ‡ã€ç½®ä¿¡åº¦ã€ç±»åˆ«ï¼‰
    :param img_w: å¸§å®½åº¦
    :param img_h: å¸§é«˜åº¦
    :return: (æ˜¯å¦åˆæ ¼, åŸå› , æ£€æµ‹æ¡†åæ ‡[x1,y1,x2,y2]æˆ–None)
    """
    # 1. ç©ºæ£€æµ‹ç»“æœç›´æ¥è¿”å›ä¸åˆæ ¼
    if len(boxes_obj) == 0:
        return False, "æœªæ£€æµ‹åˆ°äººè„¸", None
    
    # 2. ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤ä½ç½®ä¿¡åº¦äººè„¸ï¼ˆç½®ä¿¡åº¦é˜ˆå€¼ä¼˜å…ˆï¼‰
    coords = boxes_obj.xyxy.cpu().numpy()       # æ‰€æœ‰æ£€æµ‹æ¡†åæ ‡ (N,4)
    scores = boxes_obj.conf.cpu().numpy()       # æ‰€æœ‰æ£€æµ‹æ¡†ç½®ä¿¡åº¦ (N,)
    print(f"    åŸå§‹æ£€æµ‹äººè„¸æ•°: {len(scores)}, ç½®ä¿¡åº¦åˆ—è¡¨: {scores.round(2)}")
    
    # è¿‡æ»¤ä½ç½®ä¿¡åº¦äººè„¸
    conf_mask = scores >= DET_SCORE_THRESHOLD
    conf_valid_coords = coords[conf_mask]       # ç½®ä¿¡åº¦è¾¾æ ‡çš„åæ ‡
    conf_valid_scores = scores[conf_mask]       # ç½®ä¿¡åº¦è¾¾æ ‡çš„åˆ†æ•°
    if len(conf_valid_coords) == 0:
        return False, f"æ— ç½®ä¿¡åº¦è¾¾æ ‡äººè„¸ï¼ˆé˜ˆå€¼={DET_SCORE_THRESHOLD}ï¼‰", None
    
    # 3. ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤å°ºå¯¸ä¸è¾¾æ ‡äººè„¸ï¼ˆåœ¨ç½®ä¿¡åº¦åˆæ ¼çš„åŸºç¡€ä¸Šï¼‰
    size_valid_coords = []
    size_valid_scores = []
    for i in range(len(conf_valid_coords)):
        x1i, y1i, x2i, y2i = conf_valid_coords[i]
        face_wi = x2i - x1i
        face_hi = y2i - y1i
        face_w_ratioi = face_wi / img_w
        face_h_ratioi = face_hi / img_h
        
        # å°ºå¯¸è¾¾æ ‡åˆ™ä¿ç•™
        if face_w_ratioi >= FACE_SIZE_THRESHOLD and face_h_ratioi >= FACE_SIZE_THRESHOLD:
            size_valid_coords.append(conf_valid_coords[i])
            size_valid_scores.append(conf_valid_scores[i])
        else:
            print(f"    äººè„¸{i}å°ºå¯¸ä¸è¾¾æ ‡ï¼ˆé˜ˆå€¼={FACE_SIZE_THRESHOLD}ï¼‰ï¼Œè¿‡æ»¤")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆæ–¹ä¾¿åç»­å¤„ç†ï¼‰
    size_valid_coords = np.array(size_valid_coords)
    size_valid_scores = np.array(size_valid_scores)
    if len(size_valid_coords) == 0:
        return False, f"ç½®ä¿¡åº¦è¾¾æ ‡ä½†æ— å°ºå¯¸åˆæ ¼äººè„¸ï¼ˆå æ¯”é˜ˆå€¼={FACE_SIZE_THRESHOLD}ï¼‰", None
    
    # 4. ç¬¬ä¸‰æ­¥ï¼šåˆ¤æ–­äººè„¸æ•°é‡ï¼ˆæœ€ååˆ¤æ–­æ•°é‡ï¼‰
    if len(size_valid_coords) != ALLOWED_FACE_COUNT:
        return False, f"å°ºå¯¸+ç½®ä¿¡åº¦è¾¾æ ‡äººè„¸æ•°={len(size_valid_coords)}ï¼ˆè¦æ±‚{ALLOWED_FACE_COUNT}å¼ ï¼‰", None
    
    # æ‰€æœ‰æ¡ä»¶è¾¾æ ‡
    final_face = size_valid_coords[0]
    return True, "é«˜è´¨é‡äººè„¸ï¼ˆç½®ä¿¡åº¦+å°ºå¯¸+æ•°é‡å‡è¾¾æ ‡ï¼‰", final_face


def get_frame_timestamp(frame_idx: int, fps: float) -> float:
    """å°†å¸§ç´¢å¼•è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆç§’ï¼‰"""
    return frame_idx / fps


def calculate_max_bbox(bbox_list: List[np.ndarray], img_width: int, img_height: int) -> Tuple[int, int, int, int]:
    """
    è®¡ç®—ä¸€ç³»åˆ—æ£€æµ‹æ¡†çš„æœ€å¤§è¾¹ç•Œæ¡†
    :param bbox_list: æ£€æµ‹æ¡†åˆ—è¡¨ï¼Œæ¯ä¸ªä¸º[x1, y1, x2, y2]
    :param img_width: å›¾åƒå®½åº¦
    :param img_height: å›¾åƒé«˜åº¦
    :return: æœ€å¤§è¾¹ç•Œæ¡† (x1, y1, x2, y2)
    """
    if not bbox_list:
        return 0, 0, img_width, img_height
    
    # å°†æ‰€æœ‰åæ ‡å †å 
    all_coords = np.vstack(bbox_list)
    
    # è®¡ç®—æœ€å°å€¼å’Œæœ€å¤§å€¼
    min_x = int(np.min(all_coords[:, 0]))
    min_y = int(np.min(all_coords[:, 1]))
    max_x = int(np.max(all_coords[:, 2]))
    max_y = int(np.max(all_coords[:, 3]))
    
    # è®¡ç®—åŸå§‹å®½é«˜
    width = max_x - min_x
    height = max_y - min_y
    
    # æ·»åŠ å¡«å……
    padding_x = int(width * CROP_PADDING_RATIO)
    padding_y = int(height * CROP_PADDING_RATIO)
    
    # åº”ç”¨å¡«å……å¹¶ç¡®ä¿è¾¹ç•Œåœ¨å›¾åƒå†…
    x1 = max(0, min_x - padding_x)
    y1 = max(0, min_y - padding_y)
    x2 = min(img_width, max_x + padding_x)
    y2 = min(img_height, max_y + padding_y)
    
    # ç¡®ä¿æœ€å°å°ºå¯¸
    crop_width = x2 - x1
    crop_height = y2 - y1
    
    if crop_width < MIN_CROP_SIZE:
        diff = MIN_CROP_SIZE - crop_width
        x1 = max(0, x1 - diff // 2)
        x2 = min(img_width, x2 + diff - diff // 2)
    
    if crop_height < MIN_CROP_SIZE:
        diff = MIN_CROP_SIZE - crop_height
        y1 = max(0, y1 - diff // 2)
        y2 = min(img_height, y2 + diff - diff // 2)
    
    return x1, y1, x2, y2


def crop_video_by_timestamp_with_bbox(
    input_path: str, 
    output_path: str, 
    start_ts: float, 
    end_ts: float,
    bbox_list: List[np.ndarray],
    original_width: int,
    original_height: int
) -> bool:
    """
    ä½¿ç”¨ffmpegè£å‰ªè§†é¢‘å¹¶åº”ç”¨æœ€å¤§è¾¹ç•Œæ¡†è£å‰ª
    :param input_path: è¾“å…¥è§†é¢‘è·¯å¾„
    :param output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
    :param start_ts: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    :param end_ts: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    :param bbox_list: è¯¥æ—¶é—´æ®µå†…æ‰€æœ‰æ£€æµ‹æ¡†åˆ—è¡¨
    :param original_width: åŸå§‹è§†é¢‘å®½åº¦
    :param original_height: åŸå§‹è§†é¢‘é«˜åº¦
    :return: æ˜¯å¦æˆåŠŸ
    """
    duration = end_ts - start_ts
    if duration < MIN_VALID_DURATION:
        print(f"âš ï¸  ç‰‡æ®µæ—¶é•¿{duration:.2f}ç§’ < æœ€å°é˜ˆå€¼{MIN_VALID_DURATION}ç§’ï¼Œè·³è¿‡ä¿å­˜")
        return False
    
    # è®¡ç®—æœ€å¤§è¾¹ç•Œæ¡†
    x1, y1, x2, y2 = calculate_max_bbox(bbox_list, original_width, original_height)
    width = x2 - x1
    height = y2 - y1
    
    print(f"    æœ€å¤§è¾¹ç•Œæ¡†: ({x1}, {y1}, {x2}, {y2}), å°ºå¯¸: {width}x{height}")
    
    # æ„å»ºè£å‰ªå‘½ä»¤
    crop_filter = f"crop={width}:{height}:{x1}:{y1}"
    
    cmd = [
        "ffmpeg",
        "-i", input_path,
        "-ss", str(start_ts),
        "-to", str(end_ts),
        "-filter:v", crop_filter,
        "-c:v", "libx264",
        "-c:a", "aac",
        "-preset", "medium",
        "-crf", "23",
        "-y",
        "-loglevel", "error",
        output_path
    ]
    
    try:
        subprocess.run(cmd, check=True)
        print(f"âœ… ä¿å­˜è£å‰ªç‰‡æ®µï¼š{output_path}")
        print(f"   ğŸ“ è£å‰ªåŒºåŸŸ: {width}x{height}, ä½ç½®: ({x1}, {y1})")
        print(f"   â±ï¸  æ—¶é•¿: {duration:.2f}ç§’ ({start_ts:.2f}ç§’ - {end_ts:.2f}ç§’)")
        
        # ä¿å­˜è£å‰ªä¿¡æ¯åˆ°JSONæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        info_path = output_path.replace('.mp4', '_info.json')
        crop_info = {
            "original_video": input_path,
            "output_video": output_path,
            "start_time": start_ts,
            "end_time": end_ts,
            "duration": duration,
            "crop_region": {
                "x1": x1, "y1": y1, "x2": x2, "y2": y2,
                "width": width, "height": height
            },
            "num_frames_with_faces": len(bbox_list),
            "padding_ratio": CROP_PADDING_RATIO
        }
        with open(info_path, 'w') as f:
            json.dump(crop_info, f, indent=2)
        
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ è£å‰ªå¤±è´¥ï¼š{output_path}ï¼Œé”™è¯¯ï¼š{e}")
        return False


def process_video(video_path: str, output_dir: str = ".", model: YOLO = None) -> None:
    """å¤„ç†è§†é¢‘ï¼ˆæ‰¹é‡æ£€æµ‹+ç‰‡æ®µè£å‰ªï¼‰"""
    if model is None:
        model = init_yolo_model()
    
    os.makedirs(output_dir, exist_ok=True)
    video_name = Path(video_path).stem
    cap = cv2.VideoCapture(video_path)
    
    # è·å–è§†é¢‘åŸºç¡€ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS) if VIDEO_FPS == 0 else VIDEO_FPS
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total_duration = total_frames / fps if fps > 0 else 0
    
    if total_frames == 0:
        print(f"é”™è¯¯ï¼šæ— æ³•è¯»å–è§†é¢‘ {video_path}")
        return
    
    # åˆå§‹åŒ–å˜é‡
    clip_num = 0
    frame_idx = 0
    processed_frames = 0
    start_time = time.time()
    
    # ç”¨äºè·Ÿè¸ªå½“å‰ç‰‡æ®µ
    valid_clip_start_ts: Optional[float] = None
    valid_clip_bboxes: List[np.ndarray] = []  # è®°å½•å½“å‰ç‰‡æ®µçš„æ‰€æœ‰æ£€æµ‹æ¡†
    valid_clip_frame_indices: List[int] = []  # è®°å½•å½“å‰ç‰‡æ®µçš„å¸§ç´¢å¼•
    
    consecutive_invalid = 0
    batch_frames: List[np.ndarray] = []
    batch_indices: List[int] = []  # è®°å½•æ‰¹æ¬¡ä¸­å¸§çš„åŸå§‹ç´¢å¼•
    
    print(f"ğŸ“½ï¸  å¼€å§‹å¤„ç†ï¼š{video_path}")
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯ï¼šFPS={fps:.2f}, åˆ†è¾¨ç‡={width}x{height}, æ€»å¸§æ•°={total_frames}, æ€»æ—¶é•¿={total_duration:.2f}ç§’")
    print(f"âš™ï¸  é…ç½®ï¼šç½®ä¿¡åº¦={DET_SCORE_THRESHOLD}, æ‰¹é‡å¤§å°={BATCH_SIZE}, è·³å¸§æ•°={FRAME_SKIP}, æœ€å°ç‰‡æ®µæ—¶é•¿={MIN_VALID_DURATION}ç§’")
    print(f"âœ‚ï¸  è£å‰ªé…ç½®ï¼šå¡«å……æ¯”ä¾‹={CROP_PADDING_RATIO}, æœ€å°è£å‰ªå°ºå¯¸={MIN_CROP_SIZE}åƒç´ ")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # å¸§è·³è¿‡å¤„ç†
        if frame_idx % (FRAME_SKIP + 1) != 0:
            frame_idx += 1
            continue
        
        # æ”¶é›†å¸§åˆ°æ‰¹æ¬¡
        batch_frames.append(frame)
        batch_indices.append(frame_idx)
        
        # æ‰¹æ¬¡æ»¡äº†æˆ–æœ€åä¸€æ‰¹ä¸è¶³æ‰¹é‡å¤§å°æ—¶è¿›è¡Œæ£€æµ‹
        if len(batch_frames) >= BATCH_SIZE:
            # æ‰¹é‡æ£€æµ‹
            results = model(batch_frames, verbose=False)
            
            # å¤„ç†æ‰¹æ¬¡ç»“æœ
            for i, (result, frame_idx_batch) in enumerate(zip(results, batch_indices)):
                is_valid, reason, bbox = is_high_quality_face(result.boxes, width, height)
                
                # å¤„ç†ç‰‡æ®µé€»è¾‘
                current_ts = get_frame_timestamp(frame_idx_batch, fps)
                
                if is_valid:
                    consecutive_invalid = 0
                    if valid_clip_start_ts is None:
                        # å¼€å§‹æ–°çš„åˆæ ¼ç‰‡æ®µ
                        valid_clip_start_ts = current_ts
                        valid_clip_bboxes = [bbox]  # æ·»åŠ ç¬¬ä¸€ä¸ªæ£€æµ‹æ¡†
                        valid_clip_frame_indices = [frame_idx_batch]
                        print(f"ğŸ”„ å¼€å§‹åˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={valid_clip_start_ts:.2f}ç§’ï¼‰")
                    else:
                        # ç»§ç»­å½“å‰ç‰‡æ®µï¼Œè®°å½•æ£€æµ‹æ¡†
                        valid_clip_bboxes.append(bbox)
                        valid_clip_frame_indices.append(frame_idx_batch)
                else:
                    # å¤„ç†ä¸åˆæ ¼å¸§
                    if FRAME_SKIP == 0:  # é€å¸§æ¨¡å¼
                        consecutive_invalid += 1
                        if valid_clip_start_ts is not None and consecutive_invalid > TOLERANCE_FRAMES:
                            # ç»“æŸå½“å‰ç‰‡æ®µ
                            end_frame_idx = valid_clip_frame_indices[-1]  # æœ€åä¸€ä¸ªåˆæ ¼å¸§
                            end_ts = get_frame_timestamp(end_frame_idx, fps)
                            
                            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
                            output_path = os.path.join(output_dir, f"{video_name}_face_crop_{clip_num}.mp4")
                            
                            # ä½¿ç”¨æœ€å¤§è¾¹ç•Œæ¡†è£å‰ªè§†é¢‘
                            print(f"ğŸ¬ è£å‰ªç‰‡æ®µ{clip_num}ï¼šå¼€å§‹æ—¶é—´={valid_clip_start_ts:.2f}ç§’ï¼Œç»“æŸæ—¶é—´={end_ts:.2f}ç§’")
                            print(f"    åˆæ ¼å¸§æ•°ï¼š{len(valid_clip_bboxes)}ï¼Œæ£€æµ‹æ¡†æ•°é‡ï¼š{len(valid_clip_bboxes)}")
                            
                            if crop_video_by_timestamp_with_bbox(
                                video_path, output_path, valid_clip_start_ts, end_ts,
                                valid_clip_bboxes, width, height
                            ):
                                clip_num += 1
                            
                            # é‡ç½®ç‰‡æ®µçŠ¶æ€
                            valid_clip_start_ts = None
                            valid_clip_bboxes = []
                            valid_clip_frame_indices = []
                            consecutive_invalid = 0
                            print(f"ğŸ”š ç»“æŸåˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={current_ts:.2f}ç§’ï¼‰ï¼ŒåŸå› ï¼š{reason}")
                    else:  # è·³å¸§æ¨¡å¼
                        if valid_clip_start_ts is not None:
                            # ç»“æŸå½“å‰ç‰‡æ®µ
                            end_frame_idx = valid_clip_frame_indices[-1]  # æœ€åä¸€ä¸ªåˆæ ¼å¸§
                            end_ts = get_frame_timestamp(end_frame_idx, fps)
                            
                            output_path = os.path.join(output_dir, f"{video_name}_face_crop_{clip_num}.mp4")
                            
                            print(f"ğŸ¬ è£å‰ªç‰‡æ®µ{clip_num}ï¼šå¼€å§‹æ—¶é—´={valid_clip_start_ts:.2f}ç§’ï¼Œç»“æŸæ—¶é—´={end_ts:.2f}ç§’")
                            print(f"    åˆæ ¼å¸§æ•°ï¼š{len(valid_clip_bboxes)}ï¼Œæ£€æµ‹æ¡†æ•°é‡ï¼š{len(valid_clip_bboxes)}")
                            
                            if crop_video_by_timestamp_with_bbox(
                                video_path, output_path, valid_clip_start_ts, end_ts,
                                valid_clip_bboxes, width, height
                            ):
                                clip_num += 1
                            
                            valid_clip_start_ts = None
                            valid_clip_bboxes = []
                            valid_clip_frame_indices = []
                            print(f"ğŸ”š ç»“æŸåˆæ ¼ç‰‡æ®µï¼šå¸§{frame_idx_batch}ï¼ˆæ—¶é—´æˆ³={current_ts:.2f}ç§’ï¼‰ï¼ŒåŸå› ï¼š{reason}")
                
                # æ‰“å°å¸§ä¿¡æ¯
                status = "âœ…" if is_valid else "âŒ"
                if is_valid:
                    bbox_str = f" [{int(bbox[0])},{int(bbox[1])},{int(bbox[2])},{int(bbox[3])}]"
                else:
                    bbox_str = ""
                print(f"å¸§{frame_idx_batch} {status} - {reason}{bbox_str}")
                
                processed_frames += 1
                # é€Ÿåº¦ç»Ÿè®¡
                if processed_frames % SPEED_PRINT_INTERVAL == 0:
                    elapsed = time.time() - start_time
                    speed = processed_frames / elapsed
                    print(f"ğŸ“ˆ å·²å¤„ç†{processed_frames}å¸§ï¼Œé€Ÿåº¦ï¼š{speed:.2f}å¸§/ç§’")
            
            # é‡ç½®æ‰¹æ¬¡
            batch_frames = []
            batch_indices = []
        
        frame_idx += 1

    # å¤„ç†æœ€åä¸€æ‰¹å‰©ä½™å¸§
    if batch_frames:
        results = model(batch_frames, verbose=False)
        for i, (result, frame_idx_batch) in enumerate(zip(results, batch_indices)):
            is_valid, reason, bbox = is_high_quality_face(result.boxes, width, height)
            current_ts = get_frame_timestamp(frame_idx_batch, fps)
            
            if is_valid:
                if valid_clip_start_ts is None:
                    valid_clip_start_ts = current_ts
                    valid_clip_bboxes = [bbox]
                    valid_clip_frame_indices = [frame_idx_batch]
                else:
                    valid_clip_bboxes.append(bbox)
                    valid_clip_frame_indices.append(frame_idx_batch)
            
            processed_frames += 1

    # å¤„ç†æœ€åä¸€æ®µåˆæ ¼ç‰‡æ®µ
    if valid_clip_start_ts is not None and len(valid_clip_bboxes) > 0:
        end_frame_idx = valid_clip_frame_indices[-1]
        end_ts = get_frame_timestamp(end_frame_idx, fps)
        output_path = os.path.join(output_dir, f"{video_name}_face_crop_{clip_num}.mp4")
        
        print(f"ğŸ¬ è£å‰ªæœ€åä¸€æ®µç‰‡æ®µ{clip_num}ï¼šå¼€å§‹æ—¶é—´={valid_clip_start_ts:.2f}ç§’ï¼Œç»“æŸæ—¶é—´={end_ts:.2f}ç§’")
        print(f"    åˆæ ¼å¸§æ•°ï¼š{len(valid_clip_bboxes)}ï¼Œæ£€æµ‹æ¡†æ•°é‡ï¼š{len(valid_clip_bboxes)}")
        
        if crop_video_by_timestamp_with_bbox(
            video_path, output_path, valid_clip_start_ts, end_ts,
            valid_clip_bboxes, width, height
        ):
            clip_num += 1

    # æ”¶å°¾ç»Ÿè®¡
    total_elapsed = time.time() - start_time
    avg_speed = processed_frames / total_elapsed if total_elapsed > 0 else 0
    print(f"\nğŸ å¤„ç†å®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶ï¼š{total_elapsed:.2f}ç§’ï¼Œå¹³å‡é€Ÿåº¦ï¼š{avg_speed:.2f}å¸§/ç§’")
    print(f"ğŸ“¦ ç”Ÿæˆé¢éƒ¨ç‰¹å†™ç‰‡æ®µæ•°ï¼š{clip_num}")
    print(f"ğŸ“ ä¿å­˜è·¯å¾„ï¼š{os.path.abspath(output_dir)}")

    cap.release()


# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    test_video_path = "./test/24494339-1-192.mp4"
    output_directory = "./output/24494339-1-192_face_crops"
    model = init_yolo_model()
    process_video(test_video_path, output_directory, model)
